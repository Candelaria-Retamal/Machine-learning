{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609b424-ce33-44ba-a03d-99bdbd042ef1",
   "metadata": {},
   "source": [
    "### In Scikit-learn\n",
    "In this unit, we will see how to implement linear regressions with the Scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed598f1-5bee-49df-9fbf-39ad990cdadf",
   "metadata": {},
   "source": [
    "#### Linear regression\n",
    "First, let’s start by creating the X and y Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6b3949-4f7b-4b5b-8059-7d33c0eb2e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data_df = pd.read_csv(\"c3_marketing-campaign.csv\")\n",
    "X = data_df.drop(\"sales\", axis=1).values\n",
    "y = data_df.sales.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05ec61-ffb1-4031-8a0b-96a9714e38eb",
   "metadata": {},
   "source": [
    "The Scikit-learn library provides a LinearRegression object that uses the OLS method to fit a linear regression model. Just like the HuberRegressor and the SGDRegressor ones, this object implements the fit() function. In the Scikit-learn jargon, these objects are estimators and the function is part of the estimator API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b1baa6-f6cb-4253-a00d-d11673af7b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.39465146 0.47037002 0.30669954]\n",
      "Intercept: 0.024870917888196065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a linear regression object\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"Coefficients:\", lr.coef_)\n",
    "print(\"Intercept:\", lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3f830-6d42-4063-b729-effc51e62f35",
   "metadata": {},
   "source": [
    "The LinearRegression() object will automatically compute the intercept term w0. We can access the parameters with the coef_ and the intercept_ attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c7c7a-80ce-4f6d-a7f6-90af0ff12b43",
   "metadata": {},
   "source": [
    "#### Predict score functions\n",
    "Scikit-learn estimators also implement a predict() and a score() function.\n",
    "\n",
    "The predict(X) function uses the coef_ and the intercept_ attributes to compute predictions for the data points in the X matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ac4cc8-152d-4f25-9edc-1eba6cf0b892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.24462012, 4.84934038, 4.04266482])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute predictions\n",
    "y_pred = lr.predict(X)\n",
    "y_pred[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7af5d9-d50d-4d52-a8e6-859fabfaa45c",
   "metadata": {},
   "source": [
    "We can also manually compute these predictions with the matmul() function from Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1d41246-5de2-495c-a729-adae74bed1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.24462012, 4.84934038, 4.04266482])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = np.matmul(X, lr.coef_) + lr.intercept_\n",
    "y_pred[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce5151-b454-46c3-ab2a-60dd4bc82937",
   "metadata": {},
   "source": [
    "The score(X, y) function computes the predictions for the data points in X and evaluates the R2 coefficient using the target values in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc8d487-d72d-4469-99ca-64e55a9d2a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.9832893048848236\n"
     ]
    }
   ],
   "source": [
    "# Compute the R2 coefficient\n",
    "R2 = lr.score(X, y)\n",
    "print(\"R2:\", R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db01cd-171f-4038-bcf3-7134be67de8a",
   "metadata": {},
   "source": [
    "#### SGDRegressor\n",
    "\n",
    "The OLS method is the most common way to find the parameters of a linear regression model that minimize the squares of the residuals. However, in the unit about Huber loss, we saw the SGDRegressor object which implements a variant of the gradient descent (GD) algorithm.\n",
    "\n",
    "It’s important to understand that this algorithm doesn’t compute an analytical solution. It’s an iterative algorithm that tries to get closer to the optimal solution after each iteration. However, unlike the OLS method, gradient descent is very generic and can optimize many different cost functions, e.g., Huber loss.\n",
    "\n",
    "To minimize the squares of the residuals, we can set its loss parameter to squared_error. By default, it adds a penalization term to the cost function. We will learn more about it later in this course. As for now, we can set it 'none'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa966b2-0f4c-4914-a1ff-4ead4b0dadaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.39968853 0.44409771 0.25894341]\n",
      "Intercept: [0.12807209]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Create the SGDRegressor object\n",
    "lr_sgd = SGDRegressor(\n",
    "    loss=\"squared_error\",  # Cost function\n",
    "    penalty=\"none\",  # Add a penalty term?\n",
    "    max_iter=1000,  # Number of iterations\n",
    "    random_state=0,  # The implementation shuffles the data\n",
    "    tol=1e-3,  # Tolerance for improvement (stop SGD once loss is below)\n",
    ")\n",
    "\n",
    "# Fit the linear regression model\n",
    "lr_sgd.fit(X, y)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"Coefficients:\", lr_sgd.coef_)\n",
    "print(\"Intercept:\", lr_sgd.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37728b-b9f3-46c9-976d-d5d018d9c4a2",
   "metadata": {},
   "source": [
    "The implementation of the SGDRegressor object shuffles the data before running the optimization algorithm. To get the results from above, you should set its random_state parameter to zero.\n",
    "\n",
    "The object also implements the estimator API. For instance, we can compute the R2 with the score() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c13ae4-d0a1-4e00-b3df-0628b58b7759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_sgd: 0.9821546772612869\n"
     ]
    }
   ],
   "source": [
    "# Compute R2 coefficient\n",
    "R2_sgd = lr_sgd.score(X, y)\n",
    "print(\"R2_sgd:\", R2_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017dedc-1cb0-46df-b0ca-491138041eb1",
   "metadata": {},
   "source": [
    "#### Huber loss\n",
    "We usually fit linear regressions using the least squares approach, i.e., minimizing the squares of the residuals. However, it’s also possible to use other objective functions such as Huber loss.\n",
    "\n",
    "To achieve this, we can create an HuberRegressor object which also implements the estimator interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c38491-492f-4ed5-a47f-0e9458dc1b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.39172544 0.4788203  0.29315421]\n",
      "Intercept: 0.04586298819194033\n",
      "R^2 coefficient: 0.9830701571142849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "# Create the estimator\n",
    "huber = HuberRegressor(epsilon=1.35)\n",
    "\n",
    "# Fit it to X,y\n",
    "huber.fit(X, y)\n",
    "\n",
    "print(\"Coefficients:\", huber.coef_)\n",
    "print(\"Intercept:\", huber.intercept_)\n",
    "print(\"R^2 coefficient:\", huber.score(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25c2ba-f374-4898-931c-76c7912699e2",
   "metadata": {},
   "source": [
    "### Ill-conditioning\n",
    "\n",
    "In this unit, we will learn about ill-conditioning which can cause numerical issues when computing the ordinary least squares solution (OLS). We will start by describing collinearity which is a phenomenon closely related to ill-conditioning. Then, we will see how nearly collinear features can cause numerical issues. Finally, we will experiment with regularization which is a way to handle ill-conditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f095c8-4f8e-48d8-a4a5-84811d7bb1d2",
   "metadata": {},
   "source": [
    "#### Collinearity\n",
    "Collinearity (and multicollinearity) happens when there is an exact linear relationship between one or more features in the input matrix \n",
    "X. This means that one feature is a combination of the others plus some constant. This may sound abstract, so let’s take an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acf550f6-a8b4-4edc-916f-b58bbb8d343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data_df = pd.read_csv(\"c3_bike-sharing.csv\")\n",
    "\n",
    "# Create Numpy arrays\n",
    "temp = data_df.temp.values\n",
    "users = data_df.users.values\n",
    "\n",
    "# Create collinear feature\n",
    "temp_C = 47 * temp - 8\n",
    "\n",
    "\n",
    "# Create input matrix X\n",
    "X = np.c_[temp, temp_C]\n",
    "\n",
    "# Add a column of ones\n",
    "X1 = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "# Compute rank\n",
    "rank = np.linalg.matrix_rank(X1)\n",
    "print(\"Rank\", rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1509a-f896-4aef-aa77-277da506bc30",
   "metadata": {},
   "source": [
    "#### Collinearity in practice\n",
    "In theory, the OLS solution doesn’t exist when X contains collinear features. However, most machine learning tools can handle these situations and return a vector of parameters w as if there were no collinear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b89c78-640d-4aeb-8688-318ac404d7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [155.34445517  27.10638524  31.24446504]\n",
      "rank: 2\n",
      "RSS: []\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import lstsq\n",
    "\n",
    "# Compute OLS using lstsq\n",
    "w, rss, rank, _ = lstsq(X1, users)\n",
    "\n",
    "print(\"w:\", w)\n",
    "print(\"rank:\", rank)\n",
    "print(\"RSS:\", rss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c1e208-fa8f-4c37-b02f-b10ff28052ec",
   "metadata": {},
   "source": [
    "In this code, we call the lstsq() function with the rank-deficient matrix X1 which returns a set of parameters w, the rank of the matrix, and the RSS score which is an array of length zero when the matrix is rank-deficient. The function also returns the singular values of the matrix (the fourth return value), but we can discard this result by assigning it to the “throwaway” variable _."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e436bc-06cc-45c3-af5a-e69fda676a06",
   "metadata": {},
   "source": [
    "Let’s compare the performance of this model to a simple linear regression with the R2 coefficient. We can use the r2_score(y,y_pred) function from the Scikit-learn metrics module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34bb1465-bec3-44be-b8d8-013d3db503d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 normal: 0.5954233080185317\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# R^2 coefficient of simple linear regression\n",
    "coefs = np.polyfit(temp, users, deg=1)\n",
    "y_pred_normal = np.polyval(coefs, temp)\n",
    "r2_normal = r2_score(users, y_pred_normal)\n",
    "print(\"R^2 normal:\", r2_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d535e6c-7a01-4e82-ac1e-ed4535bac77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 collinear: 0.5954233080185317\n"
     ]
    }
   ],
   "source": [
    "# R^2 coefficient with collinear features\n",
    "y_pred_collinear = np.matmul(X1, w)\n",
    "r2_collinear = r2_score(users, y_pred_collinear)\n",
    "print(\"R^2 collinear:\", r2_collinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9217b-4767-4348-9034-893d5b45adab",
   "metadata": {},
   "source": [
    "#### Nearly collinear features\n",
    "Sometimes, features are highly correlated but there isn’t a perfect linear relationship between them. These are nearly collinear features.\n",
    "\n",
    "For instance, say that we measure temperatures with two different thermometers. One gives temperatures in degrees Celsius and the other in degrees Fahrenheit. To simulate this scenario, we can simply convert the temp_C values to degrees Fahrenheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7b549c2-23a5-489f-8934-395b04e14297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to degrees Celsius to Fahrenheit\n",
    "temp_F = 1.8 * temp_C + 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839ba6f-2304-451b-b223-8f3dfaaccfd7",
   "metadata": {},
   "source": [
    "It’s unlikely that the two thermometers give the same temperature values. In that sense, temp_F and temp_C are nearly collinear features. To simulate this difference, we can add a small noise to the temp_F variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f619bd6-eb93-49fd-aec6-4d2ac5d9609f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 3\n",
      "RMSE: 233.36492111651108\n",
      "w: [-2854.42227205  -137.72401678    94.19559382]\n"
     ]
    }
   ],
   "source": [
    "def compute_ols_with_noise(temp_C, users):\n",
    "\n",
    "    # Convert to degrees Fahrenheit\n",
    "    temp_F = 1.8 * temp_C + 32\n",
    "\n",
    "    # Add small variations\n",
    "    noise = np.random.normal(loc=0, scale=0.01, size=temp_F.shape)\n",
    "    temp_F += noise\n",
    "\n",
    "    # Create input matrix X\n",
    "    X = np.c_[temp_C, temp_F]\n",
    "\n",
    "    # Compute OLS using lstsq\n",
    "    X1 = np.c_[np.ones(X.shape[0]), X]  # Create X1 matrix\n",
    "    w, rss, rank, _ = lstsq(X1, users)  # OLS\n",
    "\n",
    "    return w, rss, rank\n",
    "\n",
    "\n",
    "w, rss, rank = compute_ols_with_noise(temp_C, users)\n",
    "\n",
    "print(\"rank:\", rank)  # Returns: 3\n",
    "print(\"RMSE:\", np.sqrt(rss / len(users)))  # Depends on the noise value\n",
    "print(\"w:\", w)  # Depends on the noise value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10e7ae-ed32-4513-a344-4e0b7926cde6",
   "metadata": {},
   "source": [
    "In this code, we generate a random noise using a Gaussian distribution centered at zero and with a standard deviation of 0.01.\n",
    "\n",
    "This time, X1 is full rank and the lstsq() function returns the residual sum of squares. If you run the code several times, you can see that the coefficients vary a lot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aeb18512-e56f-4360-a606-98725ea77e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run  rank  RMSE                coefficients\n",
      "0    3     233.35265611649376  [8380.88005102  494.22239743 -256.8946287 ]\n",
      "1    3     230.65134740503473  [115372.43240063   6513.04443211  -3600.58950406]\n",
      "2    3     232.9053992808012   [-47757.71083779  -2663.51856806   1497.39644333]\n",
      "3    3     233.35122334582982  [-8349.62033404  -446.8559625    265.92552603]\n",
      "4    3     232.9859176013226   [39252.97238344  2230.65105489 -1221.608021  ]\n"
     ]
    }
   ],
   "source": [
    "txt_fmt = \"{:<5}{:<6}{:<20}{:}\"\n",
    "print(txt_fmt.format(\"run\", \"rank\", \"RMSE\", \"coefficients\"))\n",
    "for i in range(5):\n",
    "    w, rss, rank = compute_ols_with_noise(temp_C, users)  # Compute OLS using lstsq\n",
    "    print(txt_fmt.format(i, rank, np.sqrt(rss / len(users)), w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7fdce-c8d4-402b-a465-98f588f21aec",
   "metadata": {},
   "source": [
    "This is due to ill-conditioning.\n",
    "#### Ill-conditioning\n",
    "\n",
    "In the example from above, X1 has full-rank and the OLS solution exists. However, it’s numerically unstable. A small change in the data produces very different coefficients. We can quantify this phenomenon with the condition number. Inverting a matrix with a large condition number is numerically unstable.\n",
    "\n",
    "We can compute the condition number of X1 with the cond() function from the Numpy linalg module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72ea98f5-37aa-47a9-b3ed-b6fa87ef7895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number: 8.720781054454994e+16\n"
     ]
    }
   ],
   "source": [
    "# Condition number\n",
    "cn = np.linalg.cond(X1)\n",
    "print(\"Condition number:\", cn)  # Depends on the noise value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68986ca5-4a9d-48f5-ba9e-0f1ad4b8b577",
   "metadata": {},
   "source": [
    "The value of cn depends on the noise in the X1 matrix from above. However, you should get a value above 200 thousand. By increasing the scale of the noise, you decrease the correlation between the two variables and you should see that it reduces the condition number.\n",
    "\n",
    "Again, ill-conditioning doesn’t necessarily affect the predictive accuracy of the model. In most cases, it will simply result in large variations in the model coefficients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c1b3d2a-9e1c-4fcc-91c5-b68874d294dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 nearly collinear: -3701.1660566799164\n"
     ]
    }
   ],
   "source": [
    "# Same with the nearly collinear matrix\n",
    "y_pred_nearcol = np.matmul(X1, w)\n",
    "r2_nearcol = r2_score(users, y_pred_nearcol)\n",
    "\n",
    "# R^2 coefficient with nearly collinear features\n",
    "print(\"R^2 nearly collinear:\", r2_nearcol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f60a41-b538-4cd8-ab98-0afae1a8f5d5",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "One way to solve ill-conditioning is to create a constraint on the coefficients. The idea is to modify the objective function and add a regularization term that penalizes large coefficients.\n",
    "\n",
    "Scikit-learn implements regularization with the Ridge estimator which is similar to the LinearRegression one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ac3e9ef-cd5e-4c34-83c0-495e3a1e0c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 7.41318097 13.5428751 ]\n",
      "Intercept: -272.8970368939782\n",
      "R^2: -3.5627161779952363\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "def compute_with_regularization(temp_C, users):\n",
    "\n",
    "    # Add small variations\n",
    "    noise = np.random.normal(loc=0, scale=0.01, size=temp_C.shape)\n",
    "    temp_F = (1.8 * temp_C + 32) + noise\n",
    "\n",
    "    # Create input matrix X\n",
    "    X = np.c_[temp_C, temp_F]\n",
    "\n",
    "    # Fit a Ridge regression\n",
    "    ridge = Ridge(alpha=100)\n",
    "    ridge.fit(X, users)\n",
    "\n",
    "    return ridge\n",
    "\n",
    "ridge = compute_with_regularization(temp_C, users)\n",
    "\n",
    "print(\"Coefficients:\", ridge.coef_)\n",
    "print(\"Intercept:\", ridge.intercept_)\n",
    "print(\"R^2:\", ridge.score(X, users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92715665-65b0-4f7e-be73-8ec93520e1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run  coefficients               intercept            R^2\n",
      "0    [7.33642015, 13.58584582]  -274.284550888916    -3.569639532624815\n",
      "1    [7.58836506, 13.44538498]  -269.78181632857707  -3.547392655357105\n",
      "2    [7.86625335, 13.29049328]  -264.80637006647     -3.522868071159177\n",
      "3    [7.6399375, 13.41573798]   -268.7982151340657   -3.542360283661595\n",
      "4    [7.22076525, 13.65002799]  -276.32792079865294  -3.5796884320989157\n"
     ]
    }
   ],
   "source": [
    "txt_fmt = \"{:<5}{:<27}{:<21}{:}\"\n",
    "print(txt_fmt.format(\"run\", \"coefficients\", \"intercept\", \"R^2\"))\n",
    "for i in range(5):\n",
    "    ridge = compute_with_regularization(temp_C, users)\n",
    "    print(txt_fmt.format(i, str(list(ridge.coef_.round(8))),\n",
    "                         ridge.intercept_, ridge.score(X, users)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2178dd0-11c6-481e-a254-0d2247e9e443",
   "metadata": {},
   "source": [
    "In this code, we create an input matrix X of nearly collinear features, and we fit the Ridge model to it. Note that we pass the X matrix to the fit() function (i.e., without the column of ones) since Scikit-learn objects automatically compute the intercept term. Also, we use an alpha parameter which controls the regularization strength. Finally, we print the model coefficients and the R2 score.\n",
    "If you run the code several times, you should see that the coefficient are more stable than before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c873450-0d45-4da0-80d8-701c47d62247",
   "metadata": {},
   "source": [
    "Let’s summarize what we’ve learned in this unit. Here are a few takeaways.\n",
    "\n",
    "* Collinearity happens when there is an exact linear relationship between one or more features. In this case, the X matrix, with the additional column of ones, and its moment matrix XtX are rank-deficient and the OLS solution doesn’t exist.\n",
    "* Nearly collinear features make computations numerically unstable which result in large variations in the model coefficients. This is called ill-conditioning. It’s numerically unstable to compute the inverse of matrices with a large condition number.\n",
    "* Regularization stabilizes model coefficients.\n",
    "Ill-conditioning is a complex topic. You can take a look at this thread if you want to learn more about it. However, note that some answers refer to models that we will see later in this course.\n",
    "\n",
    "In the next exercise, you will fit a linear regression model to a dataset with multiple features (including collinear and nearly collinear ones). This is an excellent opportunity to experiment with ill-conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041dc573-2b9a-473c-a2c1-b4ab9e2ce95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adsml] *",
   "language": "python",
   "name": "conda-env-adsml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
