{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ordinary Least Squares (OLS)\n",
    "\n",
    "Solving the equation on the gradient ∇f=0 only works for a specific class of functions f that are “flat” only at their optimal value. The good news is that the RSS cost function of a linear regression model is one of them and we can find the optimal parameters analytically by solving the equation.\n",
    "\n",
    "This analytical solution is called the ordinary least squares (OLS) solution. Here is the formula that we get when solving the equation on the gradient.\n",
    "\n",
    "w =1/(X⊺X)X⊺y\n",
    "\n",
    "Here, the X and w variables correspond to the input matrix and the vector of parameters with the additional column of ones and the intercept term w0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS on the marketing campaign dataset\n",
    "We will now implement the OLS method using the formula from above. Our goal is to find the set of optimal parameters of a linear regression model for the marketing campaign dataset.\n",
    "\n",
    "Let’s start by loading the dataset into an X and a y Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data_df = pd.read_csv(\"c3_marketing-campaign.csv\")\n",
    "X = data_df.drop(\"sales\", axis=1).values\n",
    "y = data_df.sales.values\n",
    "\n",
    "# To implement the OLS formula, we first need to create the X matrix with the additional column of ones.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create X1 matrix\n",
    "X1 = np.c_[np.ones(X.shape[0]),  # Column of ones, shape: (n,)\n",
    "           X]                    # Input matrix, shape: (n,p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [0.02487092 0.39465146 0.47037002 0.30669954]\n"
     ]
    }
   ],
   "source": [
    "# matmul() function from Numpy to compute matrix multiplications and get the transpose\n",
    "# Compute OLS solution\n",
    "XX = np.matmul(X1.T, X1)\n",
    "Xy = np.matmul(X1.T, y)\n",
    "w = np.matmul(np.linalg.inv(XX), Xy)\n",
    "\n",
    "print(\"w:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [0.02487092 0.39465146 0.47037002 0.30669954]\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "from scipy.linalg import lstsq\n",
    "\n",
    "# Verify with Scipy lstsq\n",
    "w, _, _, _ = lstsq(X1, y)\n",
    "\n",
    "print(\"w:\", w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this unit, we learned about the ordinary least squares (OLS) solution which is the standard way to find the set of parameters for linear regressions. In particular, we saw that it’s an analytical solution that minimizes the RSS measure by solving an equation on its gradient.\n",
    "\n",
    "In the next unit, we will see how to implement linear regressions with the Scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adsml] *",
   "language": "python",
   "name": "conda-env-adsml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
