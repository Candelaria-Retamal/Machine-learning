{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION NOTEBOOK\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST multilayer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - Load and preprocess data\n",
    "---\n",
    "\n",
    "> **Exercise**: Load the MNIST data. Split it into train, validation and test sets. Standardize the images. Define a `get_batches(X, y, batch_size)` function to generate random X/y batches of size `batch_size` using a Python generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "with np.load(os.path.join(\"c4_mnist-60k.npz\"), allow_pickle=False) as npz_file:\n",
    "    # Load items into a dictionary\n",
    "    mnist = dict(npz_file.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create train set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    # Convert uint8 pixel values to float\n",
    "    mnist[\"data\"].astype(np.float32),\n",
    "    mnist[\"labels\"],\n",
    "    test_size=2000,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# Create validation and test sets\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_test, y_test, test_size=1000, random_state=0\n",
    ")\n",
    "\n",
    "# Rescale input data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Valid:\", X_valid.shape, y_valid.shape)\n",
    "print(\"Test:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch generator\n",
    "def get_batches(X, y, batch_size):\n",
    "    # Shuffle X,y\n",
    "    shuffled_idx = np.arange(len(y))  # 1,2,...,n\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "\n",
    "    # Enumerate indexes by steps of batch_size\n",
    "    # i: 0, b, 2b, 3b, 4b, .. where b is the batch size\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        # Batch indexes\n",
    "        batch_idx = shuffled_idx[i : i + batch_size]\n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - Create and train a multilayer network\n",
    "---\n",
    "\n",
    "> **Exercise:** Create a multilayer neural network and train it using your batch generator. Evaluate the accuracy on the validation set after each epoch. Test different architectures and parameters. Evaluate your best network on the test set. Save the trained weights of the first fully connected layer in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define hidden layer with 64 units\n",
    "hidden_layer = tf.keras.layers.Dense(\n",
    "    units=64,\n",
    "    activation=tf.nn.relu,  # ReLU\n",
    "    kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "        scale=2, distribution=\"truncated_normal\", seed=0\n",
    "    ),\n",
    "    bias_initializer=tf.zeros_initializer(),\n",
    "    name=\"hidden\",\n",
    ")\n",
    "\n",
    "# Define output layer\n",
    "logits_layer = tf.keras.layers.Dense(\n",
    "    units=10,\n",
    "    activation=None,  # No activation function\n",
    "    kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "        scale=1, distribution=\"truncated_normal\", seed=0\n",
    "    ),\n",
    "    bias_initializer=tf.zeros_initializer(),\n",
    "    name=\"output\",\n",
    ")\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.1\n",
    ")  # increase the learning rate to 0.1\n",
    "\n",
    "# Define functions used to train the graph\n",
    "@tf.function\n",
    "def compute_logits(x):\n",
    "    hidden_output = hidden_layer(x)\n",
    "    logits = logits_layer(hidden_output)\n",
    "    return logits\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def compute_loss(y, logits):\n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    mean_ce = tf.reduce_mean(ce)\n",
    "    return mean_ce\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def compute_accuracy(y, logits):\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    acc = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "    return acc\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = compute_logits(x)\n",
    "        loss = compute_loss(y, logits)\n",
    "    # Concatenate the tarainable variables in one list usint the '+' operation on lists\n",
    "    variables = hidden_layer.trainable_variables + logits_layer.trainable_variables\n",
    "    optimizer.minimize(loss=loss, var_list=variables, tape=tape)\n",
    "    return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation accuracy\n",
    "valid_acc_values = []\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Train several epochs\n",
    "for epoch in range(15):\n",
    "    # Accuracy values (train) after each batch\n",
    "    batch_acc = []\n",
    "\n",
    "    # Get batches of data\n",
    "    for X_batch, y_batch in get_batches(X_train, y_train, 64):\n",
    "        # Run training\n",
    "        batch_logits, _ = train(X_batch, y_batch)\n",
    "\n",
    "        # Evaluate training accuracy (on current batch)\n",
    "        acc = compute_accuracy(y_batch, batch_logits)\n",
    "        batch_acc.append(acc)\n",
    "\n",
    "    # Evaluate validation accuracy (on the whole data)\n",
    "    valid_logits = compute_logits(X_valid)\n",
    "    valid_acc = compute_accuracy(y_valid, valid_logits)\n",
    "    valid_acc_values.append(valid_acc)\n",
    "\n",
    "    # Print progress\n",
    "    print(\n",
    "        \"Epoch {} - valid: {:.3f} train: {:.3f} (mean)\".format(\n",
    "            epoch + 1, valid_acc, np.mean(batch_acc)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Evaluate test accuracy after training\n",
    "test_logits = compute_logits(X_test)\n",
    "test_acc = compute_accuracy(y_test, test_logits)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))\n",
    "\n",
    "# Extract weights of the hidden layer\n",
    "W1 = hidden_layer.kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy values\n",
    "plt.plot(valid_acc_values)\n",
    "plt.title(\n",
    "    \"Validation accuracy: {:.3f} (mean last 3)\".format(np.mean(valid_acc_values[-3:]))\n",
    ")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution - Visualize weights\n",
    "---\n",
    "\n",
    "> **Exercise**: Plot the weights from the first fully connected layer (the templates) with the `imshow()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of subplots\n",
    "fig, axes = plt.subplots(nrows=8, ncols=8, figsize=(8, 8))\n",
    "\n",
    "# Remove gaps between suplots\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "# Plot the weights of the 64 hidden units\n",
    "for i, axis in enumerate(axes.flatten()):\n",
    "    # Get weights of i-th hidden unit\n",
    "    weights = W1[:, i].numpy()\n",
    "\n",
    "    # Reshape into 28 by 28 array\n",
    "    weights = weights.reshape(28, 28)\n",
    "\n",
    "    # Plot weights\n",
    "    axis.imshow(weights, cmap=plt.cm.gray_r)  # Grayscale\n",
    "    axis.get_xaxis().set_visible(False)  # Disable x-axis\n",
    "    axis.get_yaxis().set_visible(False)  # Disable y-axis\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
