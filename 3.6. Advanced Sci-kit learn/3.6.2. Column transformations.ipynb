{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfa99216-8e77-4a05-9db7-74006f77736b",
   "metadata": {},
   "source": [
    "#### Column transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74077647-298e-4ce7-b251-f6d483db3158",
   "metadata": {},
   "source": [
    "TL;DR - Perform the data preparation, exploratory data analysis (EDA), feature encoding/engineering, outliers removal in Pandas before the ML modeling as we have seen so far in this course. When the data is in a good format and ready for the ML models, convert the DataFrame to a Numpy 2d float array with .values and pass it to the sklearn estimator .fit/predict/score methods or Pipeline object if a StandardScaler is needed.\n",
    "\n",
    "Explanation: So far in this program, we have seen how to structure our data analysis into (1) data preparation (2) exploratory data analysis (EDA) and (3) machine learning parts. The first two steps (1) and (2) involve a lot of data manipulation and are done in Pandas and the last one (3) with Scikit-learn. Because ML models only work with numerical data, we usually convert our DataFrame into a Numpy float 2d array only at step (3) for sklearn estimators. It’s important to understand that the data preprocessing (1) and (2) are done in Pandas. The only exception is for common preprocessing steps that are very specific to ML such as StandardScaler or dimensionality reduction such as PCA (more about this in the next course) - those are usually encapsulated into a Pipeline object as shown in the last unit. The reason for this exception is simple: those ML operations are independent of the nature of the column unlike the data manipulation steps from (1) and (2) that are usually very specific to each variable. For instance, feature engineering and outliers removal are done in Pandas because they depend on the type of variable and their meaning ex. it doesn’t make sense to create polynomial features for categories or apply z-score outliers removal to ordinal variables or skewed ones.\n",
    "\n",
    "In this unit and the next ones: Jupyter notebooks are a great way to develop/share a data analysis pipeline and document each step with Markdown cells and plots in an iterative way. At the end of this “prototyping” work, we sometimes want to encapsulate our code from (1) and (2) into a “clean” ML pipeline. In this unit and the next ones from this Advanced Scikit-learn chapter, we will see tools to achieve this. However, note that it’s not required to use those tools - they are only helpful to do this extra step of encapsulating the Pandas code from steps (1) and (2) into Scikit-learn objects at the end of the analysis/prototyping work. For this reason, this unit and the next ones from this chapter are entirely optional. You can skip those units and start working now on the final course project. When you are happy with your work, you can optionally read this unit and the next ones and think about how you could apply those tools to your analysis. However, this is entirely optional and requires good programming/debugging experience.\n",
    "\n",
    "#### Column transformations\n",
    "In this unit, we will see how to use the ColumnTransformer object from Scikit-learn to perform a few common preprocessing steps such as ordinal and one-hot encoding.\n",
    "\n",
    "Before going into the code, it’s important to understand that this tool is part of a new workflow in Scikit-learn that tries to consolidate the data manipulation steps, usually done in Pandas, with the modeling part. As we will see in this unit and the next ones, this new Pandas/Scikit-learn workflow can be very powerful - however - Scikit-learn only provides partial support for DataFrames at the moment, so it can be difficult to model complex sequences of data manipulations with it.\n",
    "\n",
    "In such cases, don’t hesitate to do part or all of the data manipulation work in Pandas as we saw previously. Also, keep an eye on the upcoming Scikit-learn releases to see how these new features evolve.\n",
    "\n",
    "#### One-hot encoding with Scikit-learn\n",
    "Let’s start by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2356441e-146f-43c5-86df-b51f19ae9013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>yr</th>\n",
       "      <th>workingday</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>season</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.160</td>\n",
       "      <td>2011</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.363</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.249</td>\n",
       "      <td>2011</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>spring</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.248</td>\n",
       "      <td>2011</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>spring</td>\n",
       "      <td>clear</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.160</td>\n",
       "      <td>2011</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>spring</td>\n",
       "      <td>clear</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.227</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.187</td>\n",
       "      <td>2011</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>spring</td>\n",
       "      <td>clear</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    temp    hum  windspeed    yr workingday holiday  weekday  season  \\\n",
       "0  0.344  0.806      0.160  2011         no      no        6  spring   \n",
       "1  0.363  0.696      0.249  2011         no      no        0  spring   \n",
       "2  0.196  0.437      0.248  2011        yes      no        1  spring   \n",
       "3  0.200  0.590      0.160  2011        yes      no        2  spring   \n",
       "4  0.227  0.437      0.187  2011        yes      no        3  spring   \n",
       "\n",
       "  weathersit  casual  \n",
       "0     cloudy     331  \n",
       "1     cloudy     131  \n",
       "2      clear     120  \n",
       "3      clear     108  \n",
       "4      clear      82  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv(\"c3_bike-sharing-data.csv\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd71ac-e6c6-475d-8ba8-60dfea69729e",
   "metadata": {},
   "source": [
    "Scikit-learn implements a OneHotEncoder transformer to handle categorical variables. Like the other objects from Scikit-learn, it accepts array-like objects, including DataFrames, as input but always returns Numpy arrays or related objects as we are will see below.\n",
    "\n",
    "Let’s test it on our data_df DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3b3c75-b4de-4f6b-948f-107a0366a348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<731x1714 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7310 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create encoder\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit_transform(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57feea70-de35-4a7d-b1ee-5d8154f68248",
   "metadata": {},
   "source": [
    "The result can be a bit surprising at first sight: we pass a DataFrame object and get a sparse matrix with 1,714 columns! In fact, the transformer encodes all the columns from the input data, including the numerical ones. So it creates a new one-hot encoded column for each distinct value in the DataFrame.\n",
    "\n",
    "Let’s see how to fix this.\n",
    "\n",
    "#### ColumnTransformer object\n",
    "So far, we always converted the input data into Numpy arrays to avoid any issues during the ml part. However, Scikit-learn recently released a ColumnTransformer object that can apply different transformations to the columns of a Pandas DataFrame object.\n",
    "\n",
    "In our case, we can use it to apply one-hot encoding to the categorical variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "654d5c5f-e51a-4bd1-8f6d-3a1965175f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Handle categorical variables\n",
    "cat_columns = [\"yr\", \"workingday\", \"holiday\", \"weekday\", \"season\", \"weathersit\"]\n",
    "cat_transformer = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    [(\"categorical\", cat_transformer, cat_columns)], remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660587b-2f10-4fd2-a6dd-fd3c221b3acd",
   "metadata": {},
   "source": [
    "In this code, we first list the categorical columns in a cat_columns variable and create the OneHotEncoder() object. This time, we specify sparse=False when creating the encoder to get Numpy arrays instead of sparse matrices. We then create the ColumnTransformer object and specify the different transformations - one in our case - by defining (name, transformer, vars) triplets. We pass the list of categorical variables with the one-hot encoder and tell the object to leave the other columns unchanged by setting its remainder attribute to 'passthrough'.\n",
    "\n",
    "Let’s test it on our input DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ad6270-c34d-480d-a31f-580feef12261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00e+00, 0.00e+00, 1.00e+00, ..., 8.06e-01, 1.60e-01, 3.31e+02],\n",
       "       [1.00e+00, 0.00e+00, 1.00e+00, ..., 6.96e-01, 2.49e-01, 1.31e+02],\n",
       "       [1.00e+00, 0.00e+00, 0.00e+00, ..., 4.37e-01, 2.48e-01, 1.20e+02],\n",
       "       ...,\n",
       "       [0.00e+00, 1.00e+00, 1.00e+00, ..., 7.53e-01, 1.24e-01, 1.59e+02],\n",
       "       [0.00e+00, 1.00e+00, 1.00e+00, ..., 4.83e-01, 3.51e-01, 3.64e+02],\n",
       "       [0.00e+00, 1.00e+00, 0.00e+00, ..., 5.78e-01, 1.55e-01, 4.39e+02]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = preprocessor.fit_transform(data_df)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3211d36-4515-44b5-9bf6-ef6387ab4105",
   "metadata": {},
   "source": [
    "It’s important to note that we pass a DataFrame object as input and get a Numpy array. By looking at the values, we can see that the first columns correspond to the one-hot encoded columns and the last ones to the untransformed ones.\n",
    "\n",
    "Let’s check the type and size of our encoded data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c47c2da-8d42-466b-b4f1-030ed852998d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (731, 24)\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Data type: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\", encoded.shape)\n",
    "print(\"Type:\", type(encoded))\n",
    "print(\"Data type:\", encoded.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c2b6fc-5c66-4fbc-b363-3a751469ac7b",
   "metadata": {},
   "source": [
    "This time, we get a reasonable number of columns. The result is now a (731, 24) Numpy array with the usual uniform float data type.\n",
    "\n",
    "The encoded data is ready for the ML estimators, but not for additional Pandas data manipulation steps since we lost the column names in the conversion. This is why we said above that it can be complex to model sequences of data manipulation steps in Scikit-learn.\n",
    "\n",
    "If we want to convert the result back into a DataFrame, we need to use the get_feature_names_out() method from our encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bace9e8e-11af-4bdd-8e6d-96c2e58a5ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This OneHotEncoder instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cat_transformer.get_feature_names_out()\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a81fc-b14a-47a5-9409-79a90e8dc6e9",
   "metadata": {},
   "source": [
    "We get an error saying that the transformer is not fitted yet. Just like Pipeline objects, the ColumnTransformer works on copies and not on the original objects directly. To access the copies, we need to use the named_transformers_ attribute which returns the steps. This is similar to the named_steps attribute from Pipeline objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "872da001-9891-46ce-b228-22e4587e9249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categorical': OneHotEncoder(sparse=False), 'remainder': 'passthrough'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.named_transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec4bb9-781b-4753-bff7-27061c8aa753",
   "metadata": {},
   "source": [
    "To get the feature names, we simply need to retrieve the encoder copy and call its get_feature_names_out()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dab74bd-feaf-4d74-80d1-6cd464decd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yr_2011', 'yr_2012', 'workingday_no', 'workingday_yes',\n",
       "       'holiday_no', 'holiday_yes', 'weekday_0', 'weekday_1', 'weekday_2',\n",
       "       'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', 'season_fall',\n",
       "       'season_spring', 'season_summer', 'season_winter',\n",
       "       'weathersit_clear', 'weathersit_cloudy', 'weathersit_rainy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.named_transformers_[\"categorical\"].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680d0e1-e104-4354-925d-212bf47412f6",
   "metadata": {},
   "source": [
    "Scikit-learn names the columns by order: x0 corresponds to the first column in cat_columns which is yr.\n",
    "\n",
    "#### Issue with missing categories\n",
    "The one-hot encoder creates a new column for each categorical value. A common issue is to have new, previously unknown, categories in the test data. For instance, let’s see what happens if we create a new storm category for the weathersit feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f1952d6-ed68-42e1-b271-481fdda7b6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>yr</th>\n",
       "      <th>workingday</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>season</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2011</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>spring</td>\n",
       "      <td>storm</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    temp    hum  windspeed    yr workingday holiday  weekday  season  \\\n",
       "0  0.344  0.806       0.16  2011         no      no        6  spring   \n",
       "\n",
       "  weathersit  casual  \n",
       "0      storm     331  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = data_df.iloc[:1].copy()\n",
    "new_data[\"weathersit\"] = \"storm\"\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed3f57-4a57-494b-99f3-1fedc755f8c6",
   "metadata": {},
   "source": [
    "If you take a look at the column names retrieved with the get_feature_names_out() call from above, you can see that the encoder only knows about the clear, cloudy and rainy categories. Let’s see how it handles this new storm category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "122d671e-1fd5-4e50-a1d8-cab68f03d10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found unknown categories ['storm'] in column 5 during transform\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    preprocessor.transform(new_data)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933da14c-72ba-4b8a-95ec-efd8fa2d5199",
   "metadata": {},
   "source": [
    "The one-hot encoder returns an exception saying that 'storm' is an unknown value. A common practice is to simply ignore unseen values and set all the corresponding one-hot encoded variables to zero i.e. x5_clear, x5_cloudy and x5_rainy.\n",
    "\n",
    "We can specify this behavior by setting the handle_unknown attribute of our OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e7e1a27-f821-4653-a6f5-c9256272ba8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00e+00, 0.00e+00, 1.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        1.00e+00, 0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 3.44e-01, 8.06e-01, 1.60e-01, 3.31e+02]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle categorical variables\n",
    "cat_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    [(\"categorical\", cat_transformer, cat_columns)], remainder=\"passthrough\"\n",
    ")\n",
    "preprocessor.fit_transform(data_df)\n",
    "preprocessor.transform(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29af5a4-b64c-46af-81b9-36d05c242482",
   "metadata": {},
   "source": [
    "If we look at the entries with index 17, 18, 19 that correspond to the weathersit variable, we can see that they all have a value of zero.\n",
    "\n",
    "#### Ordinal encoding with Scikit-learn\n",
    "Scikit-learn also provides an OrdinalEncoder object to encode ordinal variables. It takes the list of ordinal values and encodes them using a 0 to N integer scale. Let’s test it on the weathersit variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e2edffc-195e-4570-9095-72881aeab5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Handle ordinal variables\n",
    "ord_columns = [\"weathersit\"]\n",
    "ord_transformer = OrdinalEncoder(categories=[[\"clear\", \"cloudy\", \"rainy\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3365af68-2133-440d-91b4-36215eff2b44",
   "metadata": {},
   "source": [
    "In this case, the encoder will simply map clear, cloudy and rainy to respectively 0, 1 and 2.\n",
    "\n",
    "#### FunctionTransformer object\n",
    "Ordinal and one-hot encoding are two common transformations which have their dedicated Scikit-learn transformers. However, we can also create new transformers with the FunctionTransformer object.\n",
    "\n",
    "For instance, let’s create polynomial features with continuous variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8f3cf4a-7d6c-4233-953b-583372c97308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Add polynomial features\n",
    "poly_columns = [\"temp\", \"hum\", \"windspeed\"]\n",
    "poly_transformer = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"poly\", FunctionTransformer(lambda X: np.c_[X, X ** 2, X ** 3])),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b187351-345b-4256-a91c-cb5ea247bf75",
   "metadata": {},
   "source": [
    "The FunctionTransformer takes a function to apply as a parameter. In the code from above, we create an anonymous one with the lambda notation. The function simply adds the degree 2 and 3 to the input array X with the np.c_[] concatenation operation.\n",
    "\n",
    "Note that our transformer from above is not equivalent to the PolynomialFeatures one which adds all the interaction terms in addition to the polynomial features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed34e423-1b3a-448f-b07e-967e8bc1569a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['temp', 'hum', 'windspeed', 'temp^2', 'temp hum', 'temp windspeed',\n",
       "       'hum^2', 'hum windspeed', 'windspeed^2', 'temp^3', 'temp^2 hum',\n",
       "       'temp^2 windspeed', 'temp hum^2', 'temp hum windspeed',\n",
       "       'temp windspeed^2', 'hum^3', 'hum^2 windspeed', 'hum windspeed^2',\n",
       "       'windspeed^3'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polyfeat = PolynomialFeatures(degree=3, include_bias=False)\n",
    "polyfeat.fit(data_df[poly_columns])\n",
    "polyfeat.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b68a0-8a39-4d8c-8b04-fdf98b44b848",
   "metadata": {},
   "source": [
    "As we can see, with the interaction terms, the PolynomialFeatures object creates a total of 19 features instead of just the 9 polynomial ones.\n",
    "\n",
    "#### Complete pipeline\n",
    "Let’s assemble the different transformations into a final ColumnTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99b965da-7e76-4434-9287-93ba7adfc3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(731, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"categorical\", cat_transformer, cat_columns),\n",
    "        (\"ordinal\", ord_transformer, ord_columns),\n",
    "        (\"poly\", poly_transformer, poly_columns),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "encoded = preprocessor.fit_transform(data_df)\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319452e3-fb73-4e9a-a23e-dce8dd5e3d50",
   "metadata": {},
   "source": [
    "This time, we apply the three different transformations and make sure that any additional columns, if any, are dropped by setting remainder to 'drop'.\n",
    "\n",
    "If you execute the code from above, you will probably get a FutureWarning. Scikit-learn is simply warning us that the default value for one of the object parameters will change in a future release of the library. We can ignore such warnings by adding a simplefilter using the Python warnings module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c5246c6-be1c-48ef-8f12-ff2a6de5995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ca9cf-4d94-4902-874b-f6edf357a1b3",
   "metadata": {},
   "source": [
    "Let’s encapsulate our preprocessor with a LinearRegression estimator into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9185f014-0a09-4218-a106-8df1faf59256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create Pipeline\n",
    "pipe = Pipeline([(\"preprocessor\", preprocessor), (\"regressor\", LinearRegression())])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0838f-5c3f-412e-92c4-ee0ede38c52a",
   "metadata": {},
   "source": [
    "and use the usual train/test split methodology to evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b1ec945-6f80-44fd-b8fe-1d1eaff60371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 253.03\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "\n",
    "# Split into train/test sets\n",
    "X = data_df.drop(\"casual\", axis=1)\n",
    "y = data_df.casual\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit/evaluate pipeline\n",
    "pipe.fit(X_tr, y_tr)\n",
    "print(\"MAE: {:.2f}\".format(MAE(y_te, pipe.predict(X_te))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68534a4f-8cf6-435c-9519-ef97dcc0624c",
   "metadata": {},
   "source": [
    "This time, we get a slightly better MAE score than what we obtained in the previous unit with only one-hot encoding: 253 vs. 280.\n",
    "\n",
    "#### Summary\n",
    "In this unit, we saw how to perform feature-specific preprocessing steps such as OneHotEncoder, OrdinalEncoder or any FunctionTransformer handmade ones with the ColumnTransformer object.\n",
    "\n",
    "In the next unit, we will see how to encapsulate complex transformations into a custom transformer object that can be used with the tools from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d0107-00f7-4169-b343-d6a924e1774d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adsml] *",
   "language": "python",
   "name": "conda-env-adsml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
